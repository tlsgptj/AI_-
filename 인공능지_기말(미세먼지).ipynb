{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPupDeCyZ51HJ6xq/wzXFP7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlsgptj/AI_-/blob/main/%EC%9D%B8%EA%B3%B5%EB%8A%A5%EC%A7%80_%EA%B8%B0%EB%A7%90(%EB%AF%B8%EC%84%B8%EB%A8%BC%EC%A7%80).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV6IFu3SaErO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Finddust1.csv')"
      ],
      "metadata": {
        "id": "uaC9_Bk6aj1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "UWbYKib4bOFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "Vkd64N65bQCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "lBDFmFAWbWgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "-njhxcAobaEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "5bIh7aHcbcGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#미세먼지를 예측하기 위해서 필요없는 컬럼은 다 삭제해준다.\n",
        "df = df.drop(['WestSeoul_TG_out', 'WestSeoul_TG_in',\n",
        "               'Seoul_TG_out', 'Seoul_TG_in', 'Gunja_TG_out',\n",
        "               'Gunja_TG_in', 'Localpressure_seoul', 'Localpressure_incheon',\n",
        "               'Localpressure_suwon', 'Sunshine_seoul', 'Sunshine_incheon',\n",
        "               'Sunshine_suwon', 'Snowfall_seoul', 'Snowfall_incheon',\n",
        "               'Snowfall_suwon'], axis=1, errors='ignore')"
      ],
      "metadata": {
        "id": "KcpfmZctbfaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(20)"
      ],
      "metadata": {
        "id": "a3XISPh0snAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['Rain_seoul', 'Insolation_seoul', 'Snowfal_suwon', 'Rain_ansan'], axis=1, errors='ignore')"
      ],
      "metadata": {
        "id": "5Hx5QWwuuaXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "PYaSejTDvDUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['Insolation_suwon'], axis=1, errors='ignore')"
      ],
      "metadata": {
        "id": "Gz-fTV1LvEDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "YunlYHu7vQYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "dr9S3LGOvVC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 정규화\n",
        "df[\"Ozone_seoul\"] = df[\"Ozone_seoul\"].astype(float) / abs(df[\"Ozone_seoul\"].astype(float).max())\n",
        "df[[\"Ozone_seoul\"]].head(20)"
      ],
      "metadata": {
        "id": "-ic1mUAy6PNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"seoul_PM10\"] = df[\"seoul_PM10\"].astype(float) / abs(df[\"seoul_PM10\"].astype(float).max())\n",
        "df[[\"seoul_PM10\"]].head(20)"
      ],
      "metadata": {
        "id": "Pw3dfhW87nQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Sealevelpressure_seoul\"] = df[\"Sealevelpressure_seoul\"].astype(float) / abs(df[\"Sealevelpressure_seoul\"].astype(float).max())\n",
        "df[[\"Sealevelpressure_seoul\"]].head(20)"
      ],
      "metadata": {
        "id": "aPfFkJhj8V6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Humidity_seoul\"] = df[\"Humidity_seoul\"].astype(float) / abs(df[\"Humidity_seoul\"].astype(float).max())\n",
        "df[[\"Humidity_seoul\"]].head(20)"
      ],
      "metadata": {
        "id": "RMyCRJUb8VVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Winddirection_seoul\"] = df[\"Winddirection_seoul\"].astype(float) / abs(df[\"Winddirection_seoul\"].astype(float).max())\n",
        "df[[\"Winddirection_seoul\"]].head(20)"
      ],
      "metadata": {
        "id": "v5_mpyC68UWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"suwon_PM10\"] = df[\"suwon_PM10\"].astype(float) / abs(df[\"suwon_PM10\"].astype(float).max())\n",
        "df[[\"suwon_PM10\"]].head(20)"
      ],
      "metadata": {
        "id": "78Jta5cp8xon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"incheon_PM10\"] = df[\"incheon_PM10\"].astype(float) / abs(df[\"incheon_PM10\"].astype(float).max())\n",
        "df[[\"incheon_PM10\"]].head(20)"
      ],
      "metadata": {
        "id": "GGWxPSDm9FI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ansan_PM10\"] = df[\"ansan_PM10\"].astype(float) / abs(df[\"ansan_PM10\"].astype(float).max())\n",
        "df[[\"ansan_PM10\"]].head(20)"
      ],
      "metadata": {
        "id": "oQgqL5uP9ade"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Windirection_ansan\"] = df[\"Windirection_ansan\"].astype(float) / abs(df[\"Windirection_ansan\"].astype(float).max())\n",
        "df[\"windspeed_ansan\"] = df[\"windspeed_ansan\"].astype(float) / abs(df[\"windspeed_ansan\"].astype(float).max())\n",
        "df[\"Sealevelpressure_suwon\"] = df[\"Sealevelpressure_suwon\"].astype(float) / abs(df[\"Sealevelpressure_suwon\"].astype(float).max())\n",
        "df[\"Humidity_suwon\"] = df[\"Humidity_suwon\"].astype(float) / abs(df[\"Humidity_suwon\"].astype(float).max())\n",
        "df[\"Humidity_ansan\"] = df[\"Humidity_ansan\"].astype(float) / abs(df[\"Humidity_ansan\"].astype(float).max())\n",
        "df.head()"
      ],
      "metadata": {
        "id": "dmq2KyO6-LgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['Genja_TG_in', 'Winddirection_suwon', 'Windspeed_suwon', 'Rain_suwon'], axis=1, errors='ignore')"
      ],
      "metadata": {
        "id": "lY_Ii0Fg98F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "exu_uJ6a6xOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['Snowfal_incheon'], axis = 1, errors='ignore')\n",
        "df.info()"
      ],
      "metadata": {
        "id": "RMeJGMlC_pw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA"
      ],
      "metadata": {
        "id": "dNO7BeF5JwcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "\n",
        "if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "df['year'] = df['date'].dt.year\n",
        "\n",
        "yearly_pm10 = df.groupby('year')[['seoul_PM10', 'incheon_PM10', 'suwon_PM10', 'ansan_PM10']].mean()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(yearly_pm10.index, yearly_pm10['seoul_PM10'], marker='o', label='Seoul')\n",
        "plt.plot(yearly_pm10.index, yearly_pm10['incheon_PM10'], marker='o', label='Incheon')\n",
        "plt.plot(yearly_pm10.index, yearly_pm10['suwon_PM10'], marker='o', label='Suwon')\n",
        "plt.plot(yearly_pm10.index, yearly_pm10['ansan_PM10'], marker='o', label='Ansan')\n",
        "\n",
        "plt.title('Annual PM10 Comparison of Seoul, Incheon, Suwon, and Ansan')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('PM10 Concentration')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "#연도별 각 지역의 미세먼지 농도를 비교해 본 결과 안산이 가장 높았고 그 다음 수원, 서울, 인천 순이었다."
      ],
      "metadata": {
        "id": "-gspQnsKveTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이번엔 서울지역의 연도별 오존 농도를 시각화해보기로 함\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "\n",
        "if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "df['year'] = df['date'].dt.year\n",
        "\n",
        "yearly_Oz = df.groupby('year')['Ozone_seoul'].mean()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "# 'Ozone_seoul' is not a column in yearly_Oz, it is the values of the Series\n",
        "plt.plot(yearly_Oz.index, yearly_Oz.values, marker='o', label='Ozone')\n",
        "\n",
        "plt.title('Annual Ozone Comparison of Seoul')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Ozone Concentration')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "#서울의 오존 농도는 미세먼지와 다르게 증가하는 경향을 보였다."
      ],
      "metadata": {
        "id": "QfOS8bCsv3wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이번엔 서울지역의 연도별 이산화탄소 농도를 시각화해보기로 함\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "\n",
        "if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "df['year'] = df['date'].dt.year\n",
        "\n",
        "yearly_CO = df.groupby('year')['CO_seoul'].mean()\n",
        "yearly_SO2 = df.groupby('year')['SO2_seoul'].mean()\n",
        "yearly_NO2 = df.groupby('year')['NO2_seoul'].mean()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "# 'Ozone_seoul' is not a column in yearly_Oz, it is the values of the Series\n",
        "plt.plot(yearly_CO.index, yearly_Oz.values, marker='o', label='CO')\n",
        "plt.plot(yearly_SO2.index, yearly_Oz.values, marker='o', label='SO2')\n",
        "plt.plot(yearly_NO2.index, yearly_Oz.values, marker='o', label='NO2')\n",
        "\n",
        "plt.title('Annual NO2 Comparison of Seoul')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('NO2 Concentration')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "#NO2는 해년마다 증가하는 추세였음"
      ],
      "metadata": {
        "id": "geJ1DFOH3GXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "\n",
        "if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "df['year'] = df['date'].dt.year\n",
        "\n",
        "yearly_CO = df.groupby('year')['CO_seoul'].mean()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "# 'Ozone_seoul' is not a column in yearly_Oz, it is the values of the Series\n",
        "plt.plot(yearly_CO.index, yearly_Oz.values, marker='o', label='CO')\n",
        "\n",
        "plt.title('Annual CO Comparison of Seoul')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('CO Concentration')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O6lGXZ2L_1br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_timeindex = df.set_index('date')"
      ],
      "metadata": {
        "id": "cDufJLEbDVmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data pre-processing\n",
        "# Select features (columns) to be involved intro training and predictions\n",
        "cols = list(df)[2:28]\n",
        "\n",
        "# Extract dates (will be used in visualization)\n",
        "datelist_train = list(df['date'])\n",
        "datelist_train = [date for date in datelist_train]\n",
        "\n",
        "print('Training set shape == {}'.format(df.shape))\n",
        "print('All timestamps == {}'.format(len(df)))\n",
        "print('Featured selected: {}'.format(cols))"
      ],
      "metadata": {
        "id": "Btd1_1nTAMGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training set shape == (17520, 29)\n",
        "All timestamps == 17520\n",
        "Featured selected: ['Windspeed_seoul', 'Winddirection_seoul', 'Humidity_seoul', 'Sealevelpressure_seoul', 'Ozone_seoul', 'NO2_seoul']"
      ],
      "metadata": {
        "id": "iZETZv9rD_hW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = df[cols].astype(str)\n",
        "for i in cols:\n",
        "    for j in range(0, len(dataset_train)):\n",
        "        dataset_train[i][j] = dataset_train[i][j].replace(',', '')\n",
        "\n",
        "dataset_train = dataset_train.astype(float)\n",
        "\n",
        "# Using multiple features (predictors)\n",
        "training_set = dataset_train.values\n",
        "\n",
        "print('Shape of training set == {}.'.format(training_set.shape))\n",
        "training_set"
      ],
      "metadata": {
        "id": "mBgXj1sMDNjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "training_set_scaled = sc.fit_transform(training_set)\n",
        "\n",
        "sc_predict = StandardScaler()\n",
        "sc_predict.fit_transform(training_set[:, 0:1])"
      ],
      "metadata": {
        "id": "M4lpLmpYELh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Creating a data structure with 72 timestamps and 1 output\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "n_future = 30   # Number of days we want to predict into the future\n",
        "n_past = 72     # Number of past days we want to use to predict the future\n",
        "\n",
        "# Adjust the indices to select the correct columns (2:8) for the input features\n",
        "for i in range(n_past, len(training_set_scaled) - n_future + 1):\n",
        "    X_train.append(training_set_scaled[i - n_past:i, 2:8])  # Select only the relevant columns\n",
        "    y_train.append(training_set_scaled[i + n_future - 1:i + n_future, 0])  # Assuming the first column is the target\n",
        "\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "print('X_train shape == {}.'.format(X_train.shape))\n",
        "print('y_train shape == {}.'.format(y_train.shape))"
      ],
      "metadata": {
        "id": "ZRLnQfmYEXpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv1D(filters=32, kernel_size=3,\n",
        "                           strides=1, padding=\"causal\",\n",
        "                           activation=\"relu\",\n",
        "                           input_shape=(72, 6)),  # input_shape을 (72, 6)으로 수정\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "    # tf.keras.layers.LSTM(128),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=False)),\n",
        "    # tf.keras.layers.Dropout(0.1),\n",
        "    tf.keras.layers.Dense(1),\n",
        "    # tf.keras.layers.Dense(3, kernel_initializer=tf.initializers.zeros),\n",
        "    tf.keras.layers.Lambda(lambda x: x * 200),\n",
        "    # tf.keras.layers.Reshape([24, 3])\n",
        "])\n",
        "\n",
        "# lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "#     lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-5, momentum=0.9)\n",
        "\n",
        "model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mse\"])\n"
      ],
      "metadata": {
        "id": "zxcOA83CEbQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "uFX4QcVREd3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model: \"sequential_1\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " conv1d_1 (Conv1D)           (None, 72, 32)            608       \n",
        "                                                                 \n",
        " bidirectional_2 (Bidirecti  (None, 72, 64)            16640     \n",
        " onal)                                                           \n",
        "                                                                 \n",
        " bidirectional_3 (Bidirecti  (None, 64)                24832     \n",
        " onal)                                                           \n",
        "                                                                 \n",
        " dense_1 (Dense)             (None, 1)                 65        \n",
        "                                                                 \n",
        " lambda_1 (Lambda)           (None, 1)                 0         \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 42145 (164.63 KB)\n",
        "Trainable params: 42145 (164.63 KB)\n",
        "Non-trainable params: 0 (0.00 Byte)\n",
        "_________________________________________________________________"
      ],
      "metadata": {
        "id": "l4f5CCFSEyCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Determine the correct number of features from X_train\n",
        "n_features = X_train.shape[2]\n",
        "\n",
        "# Ensure the model is defined with the correct input shape\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(n_past, n_features)))  # Use n_features here\n",
        "model.add(LSTM(units=50, return_sequences=False))\n",
        "model.add(Dense(units=1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train, y_train, epochs=50, validation_split=0.2)"
      ],
      "metadata": {
        "id": "tz94JghyE1Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate list of sequence of days for predictions\n",
        "datelist_future = pd.date_range(datelist_train[-1], periods=n_future, freq='1d').tolist()\n",
        "\n",
        "'''\n",
        "Remeber, we have datelist_train from begining.\n",
        "'''\n",
        "\n",
        "# Convert Pandas Timestamp to Datetime object (for transformation) --> FUTURE\n",
        "datelist_future_ = []\n",
        "for this_timestamp in datelist_future:\n",
        "    datelist_future_.append(this_timestamp.date())"
      ],
      "metadata": {
        "id": "nyfDUBcQFD_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datelist_train[-1]"
      ],
      "metadata": {
        "id": "PdzHcYfrYvIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datelist_future_"
      ],
      "metadata": {
        "id": "C6LEZ_UkYyZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform predictions\n",
        "predictions_future = model.predict(X_train[-n_future:])\n",
        "\n",
        "predictions_train = model.predict(X_train[n_past:])"
      ],
      "metadata": {
        "id": "0NpCZMWTY07Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse the predictions to original measurements\n",
        "\n",
        "# ---> Special function: convert  to\n",
        "def datetime_to_timestamp(x):\n",
        "    '''\n",
        "        x : a given datetime value (datetime.date)\n",
        "    '''\n",
        "    return datetime.strptime(x.strftime('%Y%m%d'), '%Y%m%d')\n",
        "\n",
        "\n",
        "y_pred_future = sc_predict.inverse_transform(predictions_future)\n",
        "y_pred_train = sc_predict.inverse_transform(predictions_train)\n",
        "\n",
        "PREDICTIONS_FUTURE = pd.DataFrame(y_pred_future, columns=['WindSpeed_Seoul']).set_index(pd.Series(datelist_future))\n",
        "PREDICTION_TRAIN = pd.DataFrame(y_pred_train, columns=['WindSpeed_Seoul']).set_index(pd.Series(datelist_train[2 * n_past + n_future -1:]))\n",
        "\n",
        "# Convert  to  for PREDCITION_TRAIN\n",
        "# PREDICTION_TRAIN.index = PREDICTION_TRAIN.index.to_series().apply(datetime_to_timestamp)"
      ],
      "metadata": {
        "id": "__3C25cbY38o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PREDICTIONS_FUTURE"
      ],
      "metadata": {
        "id": "BxBOg0etY_Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PREDICTION_TRAIN"
      ],
      "metadata": {
        "id": "BGf8ntbQZCCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set plot size\n",
        "# from pylab import rcParams\n",
        "plt.rcParams['figure.figsize'] = 14, 5\n",
        "\n",
        "# Plot parameters\n",
        "START_DATE_FOR_PLOTTING = '2017-01-01'\n",
        "\n",
        "plt.plot(PREDICTIONS_FUTURE.index, PREDICTIONS_FUTURE['WindSpeed_Seoul'], color='r', label='Windspeed_Seoul')\n",
        "plt.plot(PREDICTION_TRAIN.loc[START_DATE_FOR_PLOTTING:].index, PREDICTION_TRAIN.loc[START_DATE_FOR_PLOTTING:]['WindSpeed_Seoul'], color='orange', label='Training predictions')\n",
        "\n",
        "\n",
        "plt.axvline(x = min(PREDICTIONS_FUTURE.index), color='green', linewidth=2, linestyle='--')\n",
        "\n",
        "plt.grid(which='major', color='#cccccc', alpha=0.5)\n",
        "\n",
        "plt.legend(shadow=True)\n",
        "plt.title('Predcitions', fontsize=12)\n",
        "plt.xlabel('Timeline', fontsize=10)\n",
        "plt.ylabel('range', fontsize=10)\n",
        "# plt.xticks(rotation=45, fontsize=8)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "R9A9mFkTZI-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Learning rate not recorded in training history.\")\n",
        "# Consider plotting just the loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sQNixJcnZlMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zdNo3xYLajN6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}